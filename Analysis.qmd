---
title: "Heart Disease Key Indicators"
author: "Joe Wilder, Justin Perkins"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc-location: left
editor: visual
execute: 
  warning: false
  echo: false
theme: darkly
---

```{r}
#| label: Reticulate

library(tidyverse)
library(reticulate)
library(kableExtra)
use_virtualenv("mat434")

```

```{python}
#| label: Python Imports

from scipy.stats import randint
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.gridspec as gridspec
from matplotlib import pyplot
import seaborn as sns
import geopandas as gpd
import datetime as dt

from mpl_toolkits.axes_grid1 import make_axes_locatable

from sklearn.model_selection import train_test_split
from plotnine import ggplot, aes, labs, geom_boxplot, geom_point, geom_histogram, geom_bar, geom_density, coord_flip, facet_grid, geom_jitter
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from scipy.stats import randint

from shapely.geometry import Point
from geopandas.tools import sjoin

from plotnine import *

```

```{python}
#| label: Read in Data

heart_data = pd.read_csv("./Data/Heart_Disease_Data.csv")

```

```{python}
#| label: Read in Column Descriptions

var_list_df = pd.read_csv("./Data/Vars_With_Descriptions.txt", sep=' - ', header=None, names=['Variable', 'Description'])

```

## Background of Data Set

In this report, the primary data set is derived from the CDC's BRFSS Survey, which has been thoughtfully queried and enhanced for user-friendliness by Kaggle contributor [Kamil Pytlak](https://www.kaggle.com/kamilpytlak). The modifications made include the removal of irreverent columns, and streamlining the data set for more focused and accessible analysis.

The BRFSS Survery according to the CDC is

> \[...\] the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.

## Statement of Reason

By the end of our analysis, we hope to be able to accurately predict whether or not a patient did or did not have a heart attack. In addition, we hope to be able to identify some of the key features used in our predictions. With this information, medical professionals will be able to gauge better the risk a patient is at for having a heart attack based on a list of indicators. Or, they can input all the patient's information, and get a prediction from the trained model.

## Exploratory Data Analysis

### Data Dictionary

```{r}
#| label: Print Data Dictionary 

py$var_list_df %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))

```

------------------------------------------------------------------------

### Data Preview

Here is a preview of the data set that we are using for our analysis. Each row corresponds to a single respondent, and each column corresponds to a question on the CDC's BRFSS Survey. The intersections of each row and column indicate how the respondent answered the corresponding question.

```{r}
#| label: Print Preview of Data

py$heart_data %>%
  head(n = 3) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))

```

------------------------------------------------------------------------

### Target Variable Distribution

```{python}
#| label: Target Variable Distribution

had_heart_attack_distribution = (
ggplot(heart_data, aes(x='HadHeartAttack', fill='HadHeartAttack')) + 
  geom_bar(stat='count', position='dodge', show_legend = False) + 
  geom_text(aes(label='stat(count)'), stat='count', position=position_dodge(width=0.9), va='bottom', size=8) +
  labs(x='Had Heart Attack', y='Record Count', title='Distribution of Had Heart Attack') + 
  theme(plot_title=element_text(hjust=0.5))
)

print(had_heart_attack_distribution)

```

```{python}
#| label: Target Variable Percent of Total

class_counts = heart_data['HadHeartAttack'].value_counts()
total_samples = len(heart_data)

percentage_of_total = (class_counts / total_samples * 100).round(2)

target_percent_total_df = pd.DataFrame({
    'Count': class_counts.values,
    'Percentage of Total': percentage_of_total.apply(lambda x: f"{x:.2f}%")
})

```

------------------------------------------------------------------------

```{r}
#| label: Print Percent of Total Table

py$target_percent_total_df %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))

```

As shown above, there is not an even distribution of respondents who have or have not had a heart attack. Only 5.46% of people responded saying that they have had a heart attack in the past.

------------------------------------------------------------------------

### Predictor Variable Distributions

```{python}
#| label: State Distribution

state_counts = heart_data.groupby('State').size().reset_index(name='Count')
us_map = gpd.read_file("./Data/US_States_Map/US_States.shp")

merged_data = us_map.merge(state_counts, left_on='NAME', right_on='State', how='left')

fig, ax = plt.subplots(1, 1, figsize=(20, 15))
ax.set_xlim([-130, -65]);
ax.set_ylim([24, 50]);

us_map.plot(ax=ax, edgecolor='k', linewidth=0.5, color='lightgrey')

divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="5%", pad=0.1)
merged_data.plot(column='Count', cmap='YlOrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, cax=cax)

plt.title('Count Records\nby State')
plt.show()

```

In our data set, there are a few larger clusters of respondents when looking state by state. The state with the most responses was Washington state with a total of `r  format(max(py$state_counts$Count), scientific=F)`.

```{python}
#| label: Change Order of General Health

health_categories = ['Excellent', 'Very good', 'Good', 'Fair', 'Poor']
heart_data['GeneralHealthOrdered'] = pd.Categorical(heart_data['GeneralHealth'], ordered=True, categories=health_categories)

```

```{python}
#| label: General Health Distribution

general_health_distribution = (
ggplot(heart_data, aes(x='GeneralHealthOrdered', fill='GeneralHealthOrdered')) + 
  geom_bar(stat='count', position='dodge', show_legend = False) + 
  geom_text(aes(label='stat(count)'), stat='count', position=position_dodge(width=0.9), va='bottom', size=8) +
  labs(x='General Health', y='Record Count', title='Distribution of General Health') + 
  theme(plot_title=element_text(hjust=0.5))
)

print(general_health_distribution)

heart_data = heart_data.drop('GeneralHealthOrdered', axis=1)

```

When looking at the respondents' self-reported view of their overall health, the data more or less follows a normal distribution with a left-skew. The most common response to the question was to rate their overall health as "Very Good".

```{python}
#| label: BMI Distribution
#| layout-ncol: 2

bmi_distribution_hist = (
  ggplot(heart_data, aes(x = 'BMI', fill = 'HadHeartAttack')) + 
  geom_histogram() +
  labs(x='BMI', y='Record Count', title='Distribution of BMI by HadHeartAttack') +
  theme(plot_title=element_text(hjust=0.5))
)

bmi_distribution_dens = (
  ggplot(heart_data, aes(x = 'BMI', fill = 'HadHeartAttack')) + 
  geom_density() +
  labs(x='BMI', y='Probability Density per Unit BMI', title='Density of BMI by HadHeartAttack') +
  theme(plot_title=element_text(hjust=0.5))
)

print(bmi_distribution_hist)
print(bmi_distribution_dens)

```

```{python}
#| label: Calculate BMI Summary Statistics

bmi_average_yes_heart_attack = heart_data.query('HadHeartAttack == "Yes"').BMI.mean()
bmi_average_no_heart_attack = heart_data.query('HadHeartAttack == "No"').BMI.mean()
bmi_average = heart_data.BMI.mean()

bmi_average_difference = bmi_average_no_heart_attack - bmi_average_yes_heart_attack

bmi_std_yes_heart_attack = heart_data.query('HadHeartAttack == "Yes"').BMI.std()
bmi_std_no_heart_attack = heart_data.query('HadHeartAttack == "No"').BMI.std()
bmi_std = heart_data.BMI.std()

```

BMI, body mass index, is the summary of the relationship between a person's height and weight. It can often be an indicator of poor health and we wanted to explore its relationship with having a heart attack. When looking at the histogram, you might think that BMI has a bimodal distribution. However, the density graph shows us that it is more of a left-skewed normal distribution.

Respondents who reported not having a heart attack had an average BMI of `r round(py$bmi_average_no_heart_attack, digits=2)`, while those who reported having a heart attack had an average of `r round(py$bmi_average_yes_heart_attack, digits=2)`. That being a difference of `r round(py$bmi_average_difference, digits=2)`.

### Kurtosis For Numeric columns

```{python}
#| label: Calculate Kurtosis for Numerical Columns

heart_data_kurtosis = heart_data.kurt(axis=0, numeric_only=True)
heart_data_kurtosis.rename(index={'x': 'Kurtosis'}, inplace=True)

```

```{r}
#| label: Print Kurtosis Table

py$heart_data_kurtosis %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))

```

Kurtosis is a measure of the "tailedness" of a probability distribution. It quantifies how much a distribution deviates from a normal distribution in terms of the concentration of data in its tails. There are three main types of kurtosis:

1.  **Mesokurtic (Kurtosis = 0):** The distribution has a shape similar to a normal distribution. The tails are neither too heavy nor too light.

2.  **Leptokurtic (Positive Kurtosis):** The distribution has fatter tails and a sharper peak than a normal distribution. This indicates that there are more extreme values in the distribution, and it may have more outliers.

3.  **Platykurtic (Negative Kurtosis):** The distribution has lighter tails and a flatter peak than a normal distribution. This suggests that there are fewer extreme values, and the distribution has fewer outliers.

In our data, most of our features are leptokurtic with some scores in excess of 7. This means most of our numeric features have one or two values that appear significantly more than any other with large tails that dissipate slowly. The only feature that resembles a normal distribution is Height In Meters with a Kurtosis score of 0.0073723.

------------------------------------------------------------------------

## Statistical Modeling

### Data Cleaning

A lot of our columns are represented with yes and no values. To avoid processing these columns as categorical variables, we can convert them to numerical by having 0 represent no, and 1 represent yes. This will be easier to understand for our model.

```{python}
#| label: Data Cleaning and Mapping

boolean_columns = ['PhysicalActivities', 'HadHeartAttack', 'HadAngina', 'HadStroke', 'HadAsthma', 'HadSkinCancer', 'HadCOPD', 'HadDepressiveDisorder', 'HadKidneyDisease', 'HadArthritis', 'DeafOrHardOfHearing', 'BlindOrVisionDifficulty', 'DifficultyConcentrating', 'DifficultyWalking', 'DifficultyDressingBathing', 'DifficultyErrands', 'ChestScan', 'AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver', 'HighRiskLastYear']

for column in boolean_columns:
    heart_data[column] = heart_data[column].map({'Yes': 1, 'No': 0})
    
    
heart_data['GeneralHealth'] = heart_data['GeneralHealth'].map({'Poor': 0, 'Fair': 1, 'Good': 2, 'Very good': 3, 'Excellent': 4})

```

### Data Pre-processing

To prepare our model for training, we need to split our data into train and test splits. The training data will be used for model construction and the test data will be used to evaluate our models.

We also use stratification when we create our train test splits. While we do have a lot of data in our data set, the majority of the data is for patients who have not had heart disease. By stratifying for the HadHeartAttack column, we ensure our model has plenty of data examples from patients who do have heart disease. This allows our model to better differentiate which patients have heart disease and which ones do not.

```{python}
#| label: Train Test Split

train, test = train_test_split(heart_data, train_size = 0.1, random_state = 434, stratify = heart_data['HadHeartAttack'])

X_train = train.drop(["HadHeartAttack"], axis = 1)
y_train = train["HadHeartAttack"]
X_test = test.drop(["HadHeartAttack"], axis = 1)
y_test = test["HadHeartAttack"]

```

When building a model, we need to classify our columns as either numerical or categorical. This is because we process the data differently based on that. In our modeling pipeline, numerical variables need to be scaled, and for categorical columns, we need to use one hot encoding. Scaling is done to make it easier for our model to make distinctions between small distances. One hot encoding is done to map our categorical variables to numerical columns that our model can understand.

```{python}
#| label: Define Numeric and Categorical Columns
#| output: asis

# Separate columns into numerical and categorical
num_cols = heart_data.select_dtypes(include='number').columns.tolist()
num_cols.remove('HadHeartAttack')
cat_cols = heart_data.select_dtypes(exclude='number').columns.tolist()


# Print numerical columns
print("#### Numerical Columns")
for col in num_cols:
    print(f"* {col}")

# Print categorical columns
print("\n#### Categorical Columns")
for col in cat_cols:
    print(f"* {col}")

```

```{python}
#| label: build Pipeline Function Definition

def build_pipeline(model):
  num_pipe_rf = Pipeline([
    ("num_imputer", SimpleImputer(strategy = "median")),
    ("norm", StandardScaler())
  ])
  
  cat_pipe = Pipeline([
    ("cat_imputer", SimpleImputer(strategy = "most_frequent")),
    ("one-hot", OneHotEncoder(handle_unknown="ignore"))
  ])
  
  preprocessor_rf = ColumnTransformer([
    ("num_cols", num_pipe_rf, num_cols),
    ("cat_cols", cat_pipe, cat_cols)
  ])
  
  pipe_rf = Pipeline([
    ("preprocessor", preprocessor_rf),
    ("model", model)
  ])
  return pipe_rf


```

### Base Model Construction

There are many different models that we could use to make predictions on our data set. To figure out which model we should use, we will build baseline models with minimal training and make predictions on our data set. This will allow us to quantify which model we should use for our final predictions.

The models we have chosen to test are the logistic regression model, the random forest classifier, the decision tree classifier, and the linear discriminant analysis model. All of these models can be used to make predictions that classify if a patient has heart disease or not. It is also important to do some basic parameter training on our models to help us identify which model might perform best. Each model has a unique parameter grid that we will train over.

```{python}
#| label: Define Models and Build Pipelines

lr_clf = LogisticRegression(max_iter = 1000)
rf_clf = RandomForestClassifier()
dt_clf = DecisionTreeClassifier()
lda_clf = LinearDiscriminantAnalysis()

logistic_regression_pipe = build_pipeline(lr_clf)
random_forest_pipe = build_pipeline(rf_clf)
decision_tree_pipe = build_pipeline(dt_clf)
linear_discriminant_analysis = build_pipeline(lda_clf)

pipelines = [logistic_regression_pipe, random_forest_pipe, decision_tree_pipe, linear_discriminant_analysis]


```

```{python}
#| label: Define Model Parameter Grids

lr_param_grid = {
    'model__C': [0.001, 0.01, 0.1, 1, 10],
    'model__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'model__max_iter': [500, 1000]
}

rf_param_dist = {
    'model__n_estimators': randint(100, 200),
    'model__max_depth': randint(8, 16),
    'model__min_samples_split' : [2, 3, 4, 5],
    'model__min_samples_leaf' : [1, 2, 3, 4]
}

dt_param_dist = {
    'model__min_weight_fraction_leaf': [0.0, 0.1]
}

param_grid_lda = {
    'model__priors': [None, [0.2, 0.8], [0.5, 0.5]],
    'model__tol': [0.0001, 0.001, 0.01],
}


```

Once we have our parameter grids, we can feed them into a random search. The random search function is what will train our model. It allows us to go through and train many different models using the different combinations of parameters we have defined in our parameter grids. The output we get is the best parameter combination that was found in the training session for each model we have defined.

```{python}
#| label: Train All Models
#| output: false

# Define a list containing the hyperparameter grids
param_grids = [lr_param_grid, rf_param_dist, dt_param_dist, param_grid_lda]

# Perform RandomizedSearchCV for each pipeline
best_models = []

for i, pipeline in enumerate(pipelines):
    param_dist = param_grids[i]
    
    random_search = RandomizedSearchCV(
        pipeline, param_distributions=param_dist, n_iter=1, cv=5, scoring='accuracy', n_jobs=-1, random_state=42);
    random_search.fit(X_train, y_train)
    
    best_model = random_search.best_estimator_
    best_models.append((pipeline.named_steps['model'].__class__.__name__, best_model))
      

# Print the best models
for model_name, best_model in best_models:
    print(f"Best model for {model_name}: {best_model}")


```

Here we will now make predictions on our baseline models and decide which one we should train further. We chose to use accuracy as a prediction metric because our model is only predicting a boolean value. Because of this, we do not need anything more complicated than accuracy.

```{python}
#| label: Cross Validation For All Models
#| output: false

model_accuracies = []

for model_name, best_model in best_models:
    cv_accuracy = cross_val_score(best_model, X_train, y_train, cv=10, scoring="accuracy")
    model_accuracies.append(f"Accuracy for {model_name}: {cv_accuracy.mean()}")

```

```{python}
#| label: Print All Model Accuracies

for accuracy in model_accuracies:
  print(accuracy)
  
```

After trying all the models, it seems the Logistic Regression model has the best accuracy. Let's stick with that one and try to improve it.

We do this by increasing the number of iterations we train on and going over more params

```{python}
#| label: Updated Logistic Regression Parameter Grid
#| output: false


# Updated hyperparameter grid for Logistic Regression
lr_param_grid = {
    'model__penalty': ['l2'],
    'model__tol': [1e-4],
    'model__C': [0.001, 0.01, 0.1, 1, 10, 100],
    'model__fit_intercept': [True],
    'model__intercept_scaling': [1, 2, 3],
    'model__class_weight': [None, 'balanced'],
    'model__solver': ['lbfgs', 'sag', 'saga'],
    'model__max_iter': list(np.random.randint(100, 1000, 10)),
    'model__multi_class': ['ovr'],
}

# Build the pipeline with Logistic Regression model
lr_clf = LogisticRegression()
logistic_regression_pipe = build_pipeline(lr_clf)

random_search = RandomizedSearchCV( logistic_regression_pipe, param_distributions=lr_param_grid, n_iter=1, cv=5, scoring='accuracy', n_jobs=-1, random_state=42 );

random_search.fit(X_train, y_train)
best_model = random_search.best_estimator_


```

Finally, we can make predictions on the final model

```{python}
#| label: Cross Validation for Trained Logistic Regression
#| output: false

# Evaluate the best model
cv_accuracy = cross_val_score(best_model, X_train, y_train, cv=10, scoring="accuracy");

```

```{python}
#| label: Print Logistic Regression Accuracy

print(f"Cross-validated Accuracy for Logistic Regression: {cv_accuracy.mean()}")
mean_accuracy = cv_accuracy.mean()

```

## Conclusion

Through the use of our data set, we were able to accomplish the creation of a model with an accuracy of `r py$mean_accuracy`. This allows us to be able to make predictions on whether someone has heart disease or not. We were able to do this through the use of data visualization, data cleaning, baseline model construction, and hyperparameter tuning.
